{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RAG System Experimentation Notebook\n",
    "\n",
    "Interactive notebook for running experiments with verbose output and real-time analysis.\n",
    "\n",
    "## Features\n",
    "\n",
    "1. **Verbose output captured inline** - See router/planner/retriever decisions in real-time\n",
    "2. **Detailed JSON output** - Full config, chunk indices, distances for reproducibility\n",
    "3. **Interactive analysis** - Visualize and compare results immediately\n",
    "4. **Grid search support** - Test 270 variations efficiently\n",
    "\n",
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import json\n",
    "from pathlib import Path\n",
    "import sys\n",
    "\n",
    "# Import experiment runner\n",
    "from experiment_runner_enhanced import EnhancedExperimentRunner, VariationConfig\n",
    "from src.agents import OrchestratorConfig, RouterConfig, PlannerConfig, RetrieverConfig\n",
    "\n",
    "print(\"âœ“ Imports successful\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define Experiment Variations\n",
    "\n",
    "Customize your grid search here. Example shows comprehensive parameter sweep."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomExperimentRunner(EnhancedExperimentRunner):\n",
    "    \"\"\"Custom runner with grid search variations.\"\"\"\n",
    "    \n",
    "    def create_variations(self):\n",
    "        \"\"\"\n",
    "        Create grid search variations.\n",
    "        \n",
    "        Parameters tested:\n",
    "        - chunk_size: 400, 800, 1200, 1600, 2000\n",
    "        - top_k_per_step: 1, 3, 5\n",
    "        - chunking_strategy: page, document\n",
    "        - top_k_docs: 1, 3, 5\n",
    "        - expand_context: 0, 1, 2\n",
    "        \n",
    "        Total: 5 * 3 * 2 * 3 * 3 = 270 variations\n",
    "        \"\"\"\n",
    "        variations = []\n",
    "        \n",
    "        # Grid search\n",
    "        for i in range(5):\n",
    "            chunk_size = (i + 1) * 400\n",
    "            \n",
    "            for top_k_per_step in range(1, 6, 2):  # 1, 3, 5\n",
    "                for typ in [\"page\", \"document\"]:\n",
    "                    for docs in range(1, 6, 2):  # 1, 3, 5\n",
    "                        for wdw in [0, 1, 2]:\n",
    "                            variations.append(\n",
    "                                VariationConfig(\n",
    "                                    name=f\"chunk_{chunk_size}_topk_{top_k_per_step}_window{wdw}_docs{docs}_{typ}\",\n",
    "                                    description=(\n",
    "                                        f\"chunk_size={chunk_size}, \"\n",
    "                                        f\"top_k_per_step={top_k_per_step}, \"\n",
    "                                        f\"top_k_docs={docs}, \"\n",
    "                                        f\"chunking_strategy={typ}\"\n",
    "                                    ),\n",
    "                                    orchestrator_config=OrchestratorConfig(\n",
    "                                        model=\"llama3.2\",\n",
    "                                        temperature=0.0,\n",
    "                                        max_answer_tokens=4096,\n",
    "                                        router_config=RouterConfig(\n",
    "                                            model=\"llama3.2\",\n",
    "                                            top_k_docs=docs,\n",
    "                                            temperature=0.0,\n",
    "                                        ),\n",
    "                                        planner_config=PlannerConfig(\n",
    "                                            model=\"llama3.2\",\n",
    "                                            temperature=0.0,\n",
    "                                        ),\n",
    "                                        retriever_config=RetrieverConfig(\n",
    "                                            top_k_per_step=top_k_per_step,\n",
    "                                            chunk_size=chunk_size,\n",
    "                                            chunk_overlap=200,\n",
    "                                            expand_context=wdw,\n",
    "                                            chunking_strategy=typ,\n",
    "                                            use_hybrid=True,  # Enable hybrid search (best performer)\n",
    "                                            hybrid_alpha=0.5  # Adaptive at runtime\n",
    "                                        ),\n",
    "                                    ),\n",
    "                                )\n",
    "                            )\n",
    "        \n",
    "        return variations\n",
    "\n",
    "# Create runner\n",
    "runner = CustomExperimentRunner()\n",
    "variations = runner.create_variations()\n",
    "print(f\"âœ“ Created {len(variations)} variations\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test Single Variation (Quick Check)\n",
    "\n",
    "Test one variation first to ensure everything works before running full grid search."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load test questions\n",
    "questions_df = pd.read_csv(\"artifacts/questions.csv\")\n",
    "print(f\"Loaded {len(questions_df)} questions\\n\")\n",
    "\n",
    "# Test first variation on first question\n",
    "test_question = questions_df.iloc[0]\n",
    "test_variation = variations[0]\n",
    "\n",
    "print(f\"Testing: {test_variation.name}\")\n",
    "print(f\"Question: {test_question['question'][:100]}...\\n\")\n",
    "\n",
    "# Run with verbose=True to see all intermediate steps\n",
    "result = runner.run_single_question_detailed(\n",
    "    question_id=test_question['id'],\n",
    "    question=test_question['question'],\n",
    "    expected_output=test_question['expected_output'],\n",
    "    pdf_folder=test_question['PDF Folder'],\n",
    "    variation=test_variation,\n",
    "    verbose=True  # Shows all agent decisions\n",
    ")\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"RESULT SUMMARY\")\n",
    "print(\"=\"*70)\n",
    "print(f\"Exact match: {result.exact_match}\")\n",
    "print(f\"Partial match: {result.partial_match_score:.2%}\")\n",
    "print(f\"Execution time: {result.execution_time_seconds}s\")\n",
    "print(f\"Total chunks: {result.total_chunks_retrieved}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run Full Experiment Suite\n",
    "\n",
    "âš ï¸ **Warning**: 270 variations Ã— 2 questions = 540 runs (~10-20 hours)\n",
    "\n",
    "Consider:\n",
    "- Reducing variations for initial testing\n",
    "- Running overnight\n",
    "- Using subset of questions first"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Option 1: Run ALL variations (long-running)\n",
    "# results_df = runner.run_all_experiments(\n",
    "#     verbose=True,\n",
    "#     clean_collections=True,\n",
    "#     save_detailed_json=True\n",
    "# )\n",
    "\n",
    "# Option 2: Run subset for testing (recommended first)\n",
    "class SubsetRunner(EnhancedExperimentRunner):\n",
    "    \"\"\"Run subset of variations for quick testing.\"\"\"\n",
    "    \n",
    "    def create_variations(self):\n",
    "        # Test just a few key configurations\n",
    "        variations = []\n",
    "        \n",
    "        # Test: chunk size impact (fix other params)\n",
    "        for chunk_size in [400, 800, 1200, 1600, 2000]:\n",
    "            variations.append(\n",
    "                VariationConfig(\n",
    "                    name=f\"chunk_{chunk_size}\",\n",
    "                    description=f\"chunk_size={chunk_size} (baseline config)\",\n",
    "                    orchestrator_config=OrchestratorConfig(\n",
    "                        model=\"llama3.2\",\n",
    "                        temperature=0.0,\n",
    "                        max_answer_tokens=4096,\n",
    "                        router_config=RouterConfig(\n",
    "                            model=\"llama3.2\",\n",
    "                            top_k_docs=3,\n",
    "                            temperature=0.0,\n",
    "                        ),\n",
    "                        planner_config=PlannerConfig(\n",
    "                            model=\"llama3.2\",\n",
    "                            temperature=0.0,\n",
    "                        ),\n",
    "                        retriever_config=RetrieverConfig(\n",
    "                            top_k_per_step=5,\n",
    "                            chunk_size=chunk_size,\n",
    "                            chunk_overlap=200,\n",
    "                            expand_context=2,\n",
    "                            chunking_strategy=\"page\",\n",
    "                            use_hybrid=True,\n",
    "                            hybrid_alpha=0.5\n",
    "                        ),\n",
    "                    ),\n",
    "                )\n",
    "            )\n",
    "        \n",
    "        return variations\n",
    "\n",
    "# Run subset (5 variations Ã— 2 questions = 10 runs)\n",
    "subset_runner = SubsetRunner()\n",
    "results_df = subset_runner.run_all_experiments(\n",
    "    verbose=True,\n",
    "    clean_collections=True,\n",
    "    save_detailed_json=True\n",
    ")\n",
    "\n",
    "print(\"\\nâœ“ Experiment complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analyze Results\n",
    "\n",
    "Quick analysis of experiment results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Summary statistics\n",
    "print(\"=\"*70)\n",
    "print(\"RESULTS SUMMARY\")\n",
    "print(\"=\"*70)\n",
    "print(f\"\\nTotal runs: {len(results_df)}\")\n",
    "print(f\"Variations tested: {results_df['variation_name'].nunique()}\")\n",
    "print(f\"Questions: {results_df['question_id'].nunique()}\")\n",
    "\n",
    "# Performance by variation\n",
    "print(\"\\n\" + \"-\"*70)\n",
    "print(\"Performance by Variation\")\n",
    "print(\"-\"*70)\n",
    "\n",
    "summary = results_df.groupby('variation_name').agg({\n",
    "    'exact_match': 'sum',\n",
    "    'partial_match_score': 'mean',\n",
    "    'execution_time_seconds': 'mean',\n",
    "    'total_chunks_retrieved': 'mean'\n",
    "}).round(2)\n",
    "\n",
    "summary.columns = ['Exact Matches', 'Avg Partial', 'Avg Time (s)', 'Avg Chunks']\n",
    "summary = summary.sort_values('Avg Partial', ascending=False)\n",
    "\n",
    "print(summary)\n",
    "\n",
    "# Best performer\n",
    "best_var = summary.index[0]\n",
    "print(f\"\\nðŸ† Best performer: {best_var}\")\n",
    "print(f\"   Avg partial match: {summary.loc[best_var, 'Avg Partial']:.2%}\")\n",
    "print(f\"   Avg time: {summary.loc[best_var, 'Avg Time (s)']:.1f}s\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analyze Detailed JSON\n",
    "\n",
    "Examine chunk-level details for deep analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load detailed JSON (most recent)\n",
    "results_dir = Path(\"results\")\n",
    "json_files = sorted(results_dir.glob(\"detailed_results_*.json\"))\n",
    "\n",
    "if json_files:\n",
    "    latest_json = json_files[-1]\n",
    "    print(f\"Loading: {latest_json}\")\n",
    "    \n",
    "    with open(latest_json) as f:\n",
    "        detailed_results = json.load(f)\n",
    "    \n",
    "    print(f\"\\nLoaded {len(detailed_results['results'])} detailed results\")\n",
    "    \n",
    "    # Example: Examine first result\n",
    "    first_result = detailed_results['results'][0]\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*70)\n",
    "    print(\"EXAMPLE DETAILED RESULT\")\n",
    "    print(\"=\"*70)\n",
    "    print(f\"\\nQuestion: {first_result['question_id']}\")\n",
    "    print(f\"Variation: {first_result['variation_name']}\")\n",
    "    print(f\"\\nRouter selected: {len(first_result['router_selected_docs'])} docs\")\n",
    "    for doc in first_result['router_selected_docs']:\n",
    "        print(f\"  - {doc[:60]}...\")\n",
    "    \n",
    "    print(f\"\\nPlanner strategy: {first_result['planner_strategy']}\")\n",
    "    print(f\"Number of steps: {first_result['planner_num_steps']}\")\n",
    "    \n",
    "    print(f\"\\nRetrieval steps:\")\n",
    "    for step in first_result['retrieval_steps']:\n",
    "        print(f\"\\n  Step {step['step_number']}: {step['description']}\")\n",
    "        print(f\"  Query: {step['query']}\")\n",
    "        print(f\"  Retrieved: {step['num_chunks']} chunks\")\n",
    "        \n",
    "        # Show first chunk details\n",
    "        if step['chunks']:\n",
    "            chunk = step['chunks'][0]\n",
    "            print(f\"  Example chunk:\")\n",
    "            print(f\"    - Index: {chunk['chunk_index']}\")\n",
    "            print(f\"    - Source: {chunk['source_file'][:40]}...\")\n",
    "            print(f\"    - Page: {chunk['page_number']}\")\n",
    "            print(f\"    - Distance: {chunk['distance']:.3f}\")\n",
    "            print(f\"    - Text preview: {chunk['text_preview'][:100]}...\")\n",
    "    \n",
    "    print(f\"\\nMetrics:\")\n",
    "    print(f\"  Exact match: {first_result['exact_match']}\")\n",
    "    print(f\"  Partial match: {first_result['partial_match_score']:.2%}\")\n",
    "    print(f\"  Execution time: {first_result['execution_time_seconds']}s\")\n",
    "else:\n",
    "    print(\"No detailed JSON results found. Run experiments first.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compare Configurations\n",
    "\n",
    "Compare which chunks were retrieved for different configurations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare chunk retrieval for EF_1 across different variations\n",
    "if 'detailed_results' in locals():\n",
    "    ef1_results = [r for r in detailed_results['results'] if r['question_id'] == 'EF_1']\n",
    "    \n",
    "    print(\"=\"*70)\n",
    "    print(\"CHUNK RETRIEVAL COMPARISON - EF_1\")\n",
    "    print(\"=\"*70)\n",
    "    \n",
    "    for result in ef1_results[:3]:  # Show first 3 variations\n",
    "        print(f\"\\n{result['variation_name']}:\")\n",
    "        print(f\"  Partial match: {result['partial_match_score']:.2%}\")\n",
    "        print(f\"  Total chunks: {result['total_chunks_retrieved']}\")\n",
    "        \n",
    "        # Show chunk indices retrieved\n",
    "        chunk_indices = []\n",
    "        for step in result['retrieval_steps']:\n",
    "            for chunk in step['chunks']:\n",
    "                chunk_indices.append(chunk['chunk_index'])\n",
    "        \n",
    "        print(f\"  Chunk indices: {sorted(set(chunk_indices))}\")\n",
    "    \n",
    "    print(\"\\nâ„¹ï¸  This shows which chunks each configuration retrieved.\")\n",
    "    print(\"   Compare to understand why some configs perform better.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Export for Further Analysis\n",
    "\n",
    "Export results for analysis in other tools."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Export summary to CSV for Excel/other tools\n",
    "output_path = \"results/experiment_summary.csv\"\n",
    "results_df.to_csv(output_path, index=False)\n",
    "print(f\"âœ“ Exported summary to: {output_path}\")\n",
    "\n",
    "# Create pivot table for heatmap analysis\n",
    "if len(results_df) > 0:\n",
    "    pivot = results_df.pivot_table(\n",
    "        values='partial_match_score',\n",
    "        index='variation_name',\n",
    "        columns='question_id',\n",
    "        aggfunc='mean'\n",
    "    )\n",
    "    \n",
    "    pivot_path = \"results/experiment_pivot.csv\"\n",
    "    pivot.to_csv(pivot_path)\n",
    "    print(f\"âœ“ Exported pivot table to: {pivot_path}\")\n",
    "    print(\"\\nPivot table preview:\")\n",
    "    print(pivot)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Notes\n",
    "\n",
    "- **Verbose output**: Set `verbose=True` to see all agent decisions inline\n",
    "- **Detailed JSON**: Contains full config, chunk indices, and intermediate results\n",
    "- **Reproducibility**: Use JSON config to exactly recreate any run\n",
    "- **Analysis**: Compare chunk indices to understand performance differences\n",
    "\n",
    "## Next Steps\n",
    "\n",
    "1. Run subset experiments to identify promising parameter ranges\n",
    "2. Analyze detailed JSON to understand retrieval patterns\n",
    "3. Run full grid search on promising ranges\n",
    "4. Compare chunk-level details for best vs worst performers\n",
    "5. Document findings and update recommendations"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
