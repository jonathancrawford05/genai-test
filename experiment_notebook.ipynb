{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RAG Experimentation Notebook\n",
    "\n",
    "Comprehensive analysis of RAG configurations for PDF Q&A system.\n",
    "\n",
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from pathlib import Path\n",
    "\n",
    "# Configure plotting\n",
    "sns.set_style('whitegrid')\n",
    "plt.rcParams['figure.figsize'] = (12, 6)\n",
    "\n",
    "print(\"✓ Imports complete\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run Experiments\n",
    "\n",
    "Execute the experimentation harness with desired configurations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run experiments (uncomment to execute)\n",
    "# !python experiment.py --embeddings onnx nomic-embed-text sentence-transformers \\\n",
    "#                        --llms phi3 llama3.2 llama3.1 \\\n",
    "#                        --top-k 3 5 10 \\\n",
    "#                        --output experiment_results.json"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load experiment results\n",
    "with open('experiment_results.json', 'r') as f:\n",
    "    results = json.load(f)\n",
    "\n",
    "df = pd.DataFrame(results)\n",
    "\n",
    "print(f\"Loaded {len(df)} experiment results\")\n",
    "print(f\"Questions: {df['question_id'].unique()}\")\n",
    "print(f\"Embeddings: {df['embedding_model'].unique()}\")\n",
    "print(f\"LLMs: {df['llm_model'].unique()}\")\n",
    "print(f\"Top-K values: {sorted(df['top_k'].unique())}\")\n",
    "\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary Statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Aggregate by configuration\n",
    "config_summary = df.groupby(['embedding_model', 'llm_model', 'top_k']).agg({\n",
    "    'exact_match': 'sum',\n",
    "    'token_f1': 'mean',\n",
    "    'latency_sec': 'mean',\n",
    "    'llm_size_gb': 'first',\n",
    "    'question_id': 'count'\n",
    "}).rename(columns={'question_id': 'num_questions'})\n",
    "\n",
    "config_summary['exact_match_rate'] = config_summary['exact_match'] / config_summary['num_questions']\n",
    "config_summary = config_summary.sort_values('token_f1', ascending=False)\n",
    "\n",
    "print(\"\\nTop 10 Configurations by F1 Score:\")\n",
    "config_summary.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Embedding Model Comparison\n",
    "\n",
    "Are different embeddings producing different results?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare embeddings for same LLM and top_k\n",
    "embedding_comparison = df.groupby(['embedding_model', 'llm_model', 'top_k']).agg({\n",
    "    'token_f1': 'mean',\n",
    "    'latency_sec': 'mean'\n",
    "}).reset_index()\n",
    "\n",
    "# Pivot for easier comparison\n",
    "embedding_pivot = embedding_comparison.pivot_table(\n",
    "    index=['llm_model', 'top_k'],\n",
    "    columns='embedding_model',\n",
    "    values='token_f1'\n",
    ")\n",
    "\n",
    "print(\"\\nF1 Scores by Embedding Model:\")\n",
    "print(embedding_pivot)\n",
    "\n",
    "# Check if embeddings produce identical results\n",
    "if len(embedding_pivot.columns) > 1:\n",
    "    correlations = embedding_pivot.corr()\n",
    "    print(\"\\nCorrelation between embedding models:\")\n",
    "    print(correlations)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize embedding comparison\n",
    "fig, axes = plt.subplots(1, 2, figsize=(15, 5))\n",
    "\n",
    "# F1 scores\n",
    "embedding_f1 = df.groupby('embedding_model')['token_f1'].mean().sort_values(ascending=False)\n",
    "embedding_f1.plot(kind='bar', ax=axes[0], color='steelblue')\n",
    "axes[0].set_title('Average F1 Score by Embedding Model')\n",
    "axes[0].set_xlabel('Embedding Model')\n",
    "axes[0].set_ylabel('Avg F1 Score')\n",
    "axes[0].tick_params(axis='x', rotation=45)\n",
    "\n",
    "# Latency\n",
    "embedding_latency = df.groupby('embedding_model')['latency_sec'].mean().sort_values()\n",
    "embedding_latency.plot(kind='bar', ax=axes[1], color='coral')\n",
    "axes[1].set_title('Average Latency by Embedding Model')\n",
    "axes[1].set_xlabel('Embedding Model')\n",
    "axes[1].set_ylabel('Avg Latency (seconds)')\n",
    "axes[1].tick_params(axis='x', rotation=45)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LLM Model Comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare LLM performance\n",
    "llm_comparison = df.groupby('llm_model').agg({\n",
    "    'token_f1': 'mean',\n",
    "    'latency_sec': 'mean',\n",
    "    'llm_size_gb': 'first'\n",
    "}).sort_values('token_f1', ascending=False)\n",
    "\n",
    "print(\"\\nLLM Performance Summary:\")\n",
    "llm_comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize LLM comparison\n",
    "fig, axes = plt.subplots(2, 2, figsize=(15, 10))\n",
    "\n",
    "# F1 by LLM\n",
    "llm_f1 = df.groupby('llm_model')['token_f1'].mean().sort_values(ascending=False)\n",
    "llm_f1.plot(kind='barh', ax=axes[0, 0], color='steelblue')\n",
    "axes[0, 0].set_title('F1 Score by LLM')\n",
    "axes[0, 0].set_xlabel('Avg F1 Score')\n",
    "\n",
    "# Latency by LLM\n",
    "llm_latency = df.groupby('llm_model')['latency_sec'].mean().sort_values()\n",
    "llm_latency.plot(kind='barh', ax=axes[0, 1], color='coral')\n",
    "axes[0, 1].set_title('Latency by LLM')\n",
    "axes[0, 1].set_xlabel('Avg Latency (seconds)')\n",
    "\n",
    "# F1 vs Latency scatter\n",
    "scatter_data = df.groupby('llm_model').agg({\n",
    "    'token_f1': 'mean',\n",
    "    'latency_sec': 'mean',\n",
    "    'llm_size_gb': 'first'\n",
    "}).reset_index()\n",
    "\n",
    "axes[1, 0].scatter(scatter_data['latency_sec'], scatter_data['token_f1'], \n",
    "                   s=scatter_data['llm_size_gb']*50, alpha=0.6)\n",
    "for idx, row in scatter_data.iterrows():\n",
    "    axes[1, 0].annotate(row['llm_model'], \n",
    "                        (row['latency_sec'], row['token_f1']),\n",
    "                        fontsize=8)\n",
    "axes[1, 0].set_xlabel('Latency (seconds)')\n",
    "axes[1, 0].set_ylabel('F1 Score')\n",
    "axes[1, 0].set_title('F1 vs Latency (bubble size = model size)')\n",
    "axes[1, 0].grid(True, alpha=0.3)\n",
    "\n",
    "# Efficiency (F1 / latency)\n",
    "scatter_data['efficiency'] = scatter_data['token_f1'] / scatter_data['latency_sec']\n",
    "scatter_data.sort_values('efficiency', ascending=False)['efficiency'].plot(\n",
    "    kind='barh', ax=axes[1, 1], color='green'\n",
    ")\n",
    "axes[1, 1].set_title('Efficiency (F1 / Latency)')\n",
    "axes[1, 1].set_xlabel('Efficiency Score')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Top-K Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Impact of top_k on performance\n",
    "topk_analysis = df.groupby(['top_k', 'llm_model']).agg({\n",
    "    'token_f1': 'mean',\n",
    "    'latency_sec': 'mean'\n",
    "}).reset_index()\n",
    "\n",
    "fig, axes = plt.subplots(1, 2, figsize=(15, 5))\n",
    "\n",
    "# F1 vs top_k\n",
    "for llm in topk_analysis['llm_model'].unique():\n",
    "    data = topk_analysis[topk_analysis['llm_model'] == llm]\n",
    "    axes[0].plot(data['top_k'], data['token_f1'], marker='o', label=llm)\n",
    "axes[0].set_xlabel('Top-K')\n",
    "axes[0].set_ylabel('F1 Score')\n",
    "axes[0].set_title('F1 Score vs Top-K by LLM')\n",
    "axes[0].legend()\n",
    "axes[0].grid(True, alpha=0.3)\n",
    "\n",
    "# Latency vs top_k\n",
    "for llm in topk_analysis['llm_model'].unique():\n",
    "    data = topk_analysis[topk_analysis['llm_model'] == llm]\n",
    "    axes[1].plot(data['top_k'], data['latency_sec'], marker='o', label=llm)\n",
    "axes[1].set_xlabel('Top-K')\n",
    "axes[1].set_ylabel('Latency (seconds)')\n",
    "axes[1].set_title('Latency vs Top-K by LLM')\n",
    "axes[1].legend()\n",
    "axes[1].grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Per-Question Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze each question separately\n",
    "for qid in df['question_id'].unique():\n",
    "    print(f\"\\n{'=' * 80}\")\n",
    "    print(f\"Question: {qid}\")\n",
    "    print(f\"{'=' * 80}\")\n",
    "    \n",
    "    q_data = df[df['question_id'] == qid]\n",
    "    \n",
    "    # Show question and expected answer\n",
    "    print(f\"\\nQ: {q_data.iloc[0]['question']}\")\n",
    "    print(f\"\\nExpected: {q_data.iloc[0]['expected'][:200]}...\")\n",
    "    \n",
    "    # Best performers for this question\n",
    "    best = q_data.nlargest(5, 'token_f1')[[\n",
    "        'embedding_model', 'llm_model', 'top_k', 'token_f1', 'latency_sec', 'llm_size_gb'\n",
    "    ]]\n",
    "    \n",
    "    print(\"\\nTop 5 Configurations:\")\n",
    "    print(best.to_string())\n",
    "    \n",
    "    # Show actual answer from best config\n",
    "    best_answer = q_data.loc[q_data['token_f1'].idxmax(), 'answer']\n",
    "    print(f\"\\nBest Answer ({q_data.loc[q_data['token_f1'].idxmax(), 'config']}):\")\n",
    "    print(best_answer[:300] + \"...\" if len(best_answer) > 300 else best_answer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Diagnostic: Low F1 Scores\n",
    "\n",
    "Investigate why F1 scores are low."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sample a few results to inspect\n",
    "print(\"Sample Results for Inspection:\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "for idx in df.sample(min(3, len(df))).index:\n",
    "    row = df.loc[idx]\n",
    "    print(f\"\\nConfig: {row['config']}\")\n",
    "    print(f\"Question: {row['question']}\")\n",
    "    print(f\"\\nExpected:\\n{row['expected']}\")\n",
    "    print(f\"\\nActual:\\n{row['answer']}\")\n",
    "    print(f\"\\nF1: {row['token_f1']:.3f}\")\n",
    "    print(\"-\" * 80)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Recommendations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"RECOMMENDATIONS\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# Best overall\n",
    "best_overall = config_summary.iloc[0]\n",
    "print(f\"\\n1. Best Overall Configuration:\")\n",
    "print(f\"   Embedding: {best_overall.name[0]}\")\n",
    "print(f\"   LLM: {best_overall.name[1]}\")\n",
    "print(f\"   Top-K: {best_overall.name[2]}\")\n",
    "print(f\"   Avg F1: {best_overall['token_f1']:.3f}\")\n",
    "print(f\"   Avg Latency: {best_overall['latency_sec']:.2f}s\")\n",
    "\n",
    "# Fastest acceptable quality\n",
    "threshold_f1 = df['token_f1'].quantile(0.9)  # Top 10% F1\n",
    "fast_and_good = config_summary[\n",
    "    config_summary['token_f1'] >= threshold_f1\n",
    "].sort_values('latency_sec').iloc[0]\n",
    "\n",
    "print(f\"\\n2. Fastest with Good Quality:\")\n",
    "print(f\"   Embedding: {fast_and_good.name[0]}\")\n",
    "print(f\"   LLM: {fast_and_good.name[1]}\")\n",
    "print(f\"   Top-K: {fast_and_good.name[2]}\")\n",
    "print(f\"   Avg F1: {fast_and_good['token_f1']:.3f}\")\n",
    "print(f\"   Avg Latency: {fast_and_good['latency_sec']:.2f}s\")\n",
    "\n",
    "# Most efficient\n",
    "config_summary['efficiency'] = config_summary['token_f1'] / config_summary['latency_sec']\n",
    "most_efficient = config_summary.sort_values('efficiency', ascending=False).iloc[0]\n",
    "\n",
    "print(f\"\\n3. Most Efficient (F1/latency):\")\n",
    "print(f\"   Embedding: {most_efficient.name[0]}\")\n",
    "print(f\"   LLM: {most_efficient.name[1]}\")\n",
    "print(f\"   Top-K: {most_efficient.name[2]}\")\n",
    "print(f\"   Efficiency: {most_efficient['efficiency']:.4f}\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Export Detailed Results\n",
    "\n",
    "Save comprehensive CSV for further analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Export to CSV\n",
    "output_file = 'experiment_results_detailed.csv'\n",
    "df.to_csv(output_file, index=False)\n",
    "print(f\"✓ Detailed results exported to: {output_file}\")\n",
    "\n",
    "# Also export configuration summary\n",
    "config_summary.to_csv('experiment_summary.csv')\n",
    "print(f\"✓ Configuration summary exported to: experiment_summary.csv\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
